<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="m0re"><meta name="copyright" content="m0re"><meta name="generator" content="Hexo 5.1.0"><meta name="theme" content="hexo-theme-yun"><title>python分布式爬虫————爬虫前奏和网络请求 | m0re的小站</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="none" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/star-markdown-css@0.1.19/dist/yun/yun-markdown.min.css"><script src="//at.alicdn.com/t/font_1140697_stqaphw3j4.js" async></script><script src="https://cdn.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", () => {
  [".post-card",".post-content img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
});
</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script>document.addEventListener("DOMContentLoaded", function() {
  renderMathInElement(document.body, {
    delimiters: [
      {left: "$$", right: "$$", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\(", right: "\\)", display: false},
      {left: "\\[", right: "\\]", display: true}
    ]
  });
});</script><link rel="shortcut icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"root":"/","title":"人生路漫漫","version":"0.9.7","anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"local_search":{"path":"/search.xml"},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><script>(function(){
  var bp = document.createElement('script');
  var curProtocol = window.location.protocol.split(':')[0];
  if (curProtocol === 'https') {
    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else {
    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(bp, s);
})();</script><meta name="description" content="前言继续python的学习，这次学习爬虫，听说比较好玩，我也学学。 爬虫前奏什么是网络爬虫？ 爬虫的实际例子：   搜索引擎(百度、谷歌、360搜索等) 数据分析与研究 抢票软件等   什么是网络爬虫   通俗理解：爬虫是一个模拟人类请求网站行为的程序，可以自动请求网页，并将数据提取下来，然后使用一定的规则提取有价值的数据。 专业介绍：🍗百度百科🍖维基百科   通用爬虫和聚焦爬虫   通用爬虫">
<meta property="og:type" content="article">
<meta property="og:title" content="python分布式爬虫————爬虫前奏和网络请求">
<meta property="og:url" content="https://m0re.top/posts/7bd464e8/index.html">
<meta property="og:site_name" content="m0re的小站">
<meta property="og:description" content="前言继续python的学习，这次学习爬虫，听说比较好玩，我也学学。 爬虫前奏什么是网络爬虫？ 爬虫的实际例子：   搜索引擎(百度、谷歌、360搜索等) 数据分析与研究 抢票软件等   什么是网络爬虫   通俗理解：爬虫是一个模拟人类请求网站行为的程序，可以自动请求网页，并将数据提取下来，然后使用一定的规则提取有价值的数据。 专业介绍：🍗百度百科🍖维基百科   通用爬虫和聚焦爬虫   通用爬虫">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200712121120827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020071213080144.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200712131221612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200712134642641.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200712135413295.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200712140303705.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200712143038154.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200714172731396.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200714172919237.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200715184453382.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1ODM2NDc0,size_16,color_1FFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200715190503345.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200715180716730.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70">
<meta property="article:published_time" content="2020-08-23T08:31:57.606Z">
<meta property="article:modified_time" content="2020-09-01T07:51:18.690Z">
<meta property="article:author" content="m0re">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20200712121120827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70"><script src="/js/ui/mode.js"></script></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="m0re"><img width="96" loading="lazy" src="/img/avatar.jpg" alt="m0re"></a><div class="site-author-name"><a href="/about/">m0re</a></div><a class="site-name" href="/about/site.html">m0re的小站</a><sub class="site-subtitle"></sub><div class="site-desciption"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">74</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">3</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">26</span></a></div><a class="site-state-item hty-icon-button" href="/Tools/" title="工具"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-settings-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/zxcv0221" title="GitHub" target="_blank" style="color:#6e5494"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://blog.csdn.net/qq_45836474" title="CSDN" target="_blank" style="color:#E6162D"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-telegram-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://www.cnblogs.com/Augenstern-blog/" title="博客园" target="_blank" style="color:#007722"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-user-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://twitter.com/ISm0r3" title="Twitter" target="_blank" style="color:#1da1f2"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-twitter-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:wfulaj4y@gmail.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="我的小伙伴们" style="color:dodgerblue"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-genderless-line"></use></svg></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-contrast-2-line"></use></svg></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E5%89%8D%E5%A5%8F"><span class="toc-number">2.</span> <span class="toc-text">爬虫前奏</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%EF%BC%9F"><span class="toc-number">2.1.</span> <span class="toc-text">什么是网络爬虫？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82"><span class="toc-number">3.</span> <span class="toc-text">网络请求</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#urlopen%E5%87%BD%E6%95%B0%E7%94%A8%E6%B3%95"><span class="toc-number">3.1.</span> <span class="toc-text">urlopen函数用法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#urlretrieve%E5%87%BD%E6%95%B0%E7%94%A8%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">urlretrieve函数用法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%A7%A3%E7%A0%81%E5%92%8C%E8%A7%A3%E7%A0%81%E5%87%BD%E6%95%B0"><span class="toc-number">3.3.</span> <span class="toc-text">参数解码和解码函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#urlparse%E5%92%8Curlsplit"><span class="toc-number">3.4.</span> <span class="toc-text">urlparse和urlsplit</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Request%E7%B1%BB"><span class="toc-number">3.5.</span> <span class="toc-text">Request类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ProxyHandler%E5%A4%84%E7%90%86%E5%99%A8%EF%BC%88%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE%EF%BC%89"><span class="toc-number">3.6.</span> <span class="toc-text">ProxyHandler处理器（代理设置）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cookie%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86"><span class="toc-number">3.7.</span> <span class="toc-text">Cookie模拟登陆</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E8%87%AA%E5%8A%A8%E7%99%BB%E5%BD%95%E8%AE%BF%E9%97%AE%E6%8E%88%E6%9D%83%E9%A1%B5%E9%9D%A2"><span class="toc-number">3.8.</span> <span class="toc-text">爬虫自动登录访问授权页面</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="https://m0re.top/posts/7bd464e8/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="m0re"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="m0re的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">python分布式爬虫————爬虫前奏和网络请求</h1><div class="post-meta"><div class="post-time" style="display:inline-block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2020-08-23 16:31:57" itemprop="dateCreated datePublished" datetime="2020-08-23T16:31:57+08:00">2020-08-23</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-2-line"></use></svg></span> <time title="修改时间：2020-09-01 15:51:18" itemprop="dateModified" datetime="2020-09-01T15:51:18+08:00">2020-09-01</time></div><span class="leancloud_visitors" id="/posts/7bd464e8/" data-flag-title="python分布式爬虫————爬虫前奏和网络请求"><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span class="leancloud-visitors-count"></span></span></span><div class="post-classify"><span class="post-tag"><a class="tag" href="/tags/Python/" style="--text-color:#3776ab"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">Python</span></a><a class="tag" href="/tags/%E7%88%AC%E8%99%AB/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">爬虫</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body" style="--smc-primary:#0078E7;"><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>继续python的学习，这次学习爬虫，听说比较好玩，我也学学。</p>
<h1 id="爬虫前奏"><a href="#爬虫前奏" class="headerlink" title="爬虫前奏"></a>爬虫前奏</h1><h2 id="什么是网络爬虫？"><a href="#什么是网络爬虫？" class="headerlink" title="什么是网络爬虫？"></a>什么是网络爬虫？</h2><ol>
<li>爬虫的实际例子：</li>
</ol>
<ul>
<li>搜索引擎(百度、谷歌、360搜索等)</li>
<li>数据分析与研究</li>
<li>抢票软件等</li>
</ul>
<ol start="2">
<li>什么是网络爬虫</li>
</ol>
<ul>
<li>通俗理解：爬虫是一个模拟人类请求网站行为的程序，可以自动请求网页，并将数据提取下来，然后使用一定的规则提取有价值的数据。</li>
<li>专业介绍：🍗<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711?fromtitle=%E7%88%AC%E8%99%AB&fromid=22046949&fr=aladdin">百度百科</a>🍖<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%B6%B2%E8%B7%AF%E7%88%AC%E8%9F%B2">维基百科</a></li>
</ul>
<ol start="3">
<li>通用爬虫和聚焦爬虫</li>
</ol>
<ul>
<li>通用爬虫：通用爬虫是搜索引擎提取系统（百度等）的重要组成部分。主要是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。</li>
<li>聚焦爬虫：是面向特定需求的一种网络爬虫程序，他与通用爬虫的区别就在于：聚焦爬虫在实践网页抓取的时候会对内容进行筛选和处理，尽量保证只抓取与需求相关的网页信息。</li>
</ul>
<ol start="4">
<li>大部分语言基本上都可以来写爬虫，但是python有它独特的优点。</li>
</ol>
<h1 id="网络请求"><a href="#网络请求" class="headerlink" title="网络请求"></a>网络请求</h1><h2 id="urlopen函数用法"><a href="#urlopen函数用法" class="headerlink" title="urlopen函数用法"></a>urlopen函数用法</h2><p>urllib库是python中最基本的网络请求库。可以模仿浏览器的行为，向指定的服务器发送一个请求，并可以保存服务器返回的数据。</p>
<p>在Python3的urllib库中，所有和网络请求相关的方法，都被集成到<code>urllib.request</code>模块下面了。基本使用栗子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line">resp = request.urlopen(<span class="string">&quot;http://www.baidu.com&quot;</span>)</span><br><span class="line">print(resp.read())</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200712121120827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70" alt="m0re" loading="lazy"><br>实际上，使用浏览器访问百度，右键查看源代码，会发现，跟上面打印的数据是一样的。也就是说上面三行代码已经帮我将百度首页的全部代码爬下来了。<br><code>urlopen</code>函数的详细介绍：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">url:		请求的url</span><br><span class="line">data:		请求的data，如果设置了这个值，那么将变成post请求。</span><br><span class="line">返回值:		返回值是一个http.client.HTTPResponse对象，这个对象是一个类文件句柄对象。有read(size)、 readline、 readlines以及getcode等方法。</span><br></pre></td></tr></table></figure>
<p>getcode就是获取当前响应的这个状态码。</p>
<h2 id="urlretrieve函数用法"><a href="#urlretrieve函数用法" class="headerlink" title="urlretrieve函数用法"></a>urlretrieve函数用法</h2><p>这个函数可以方便的将网页上的一个文件保存到本地，以下代码可以非常方便的将百度的首页下载到本地：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line">request.urlretrieve(<span class="string">&#x27;http://www.baidu.com/&#x27;</span>,<span class="string">&#x27;baidu.html&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/2020071213080144.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70" alt="m0re" loading="lazy"><br>这个就是爬取下来的baidu页面<br>还可以爬取图片<br>随便找张图片，复制它的地址，然后修改参数，进行爬取<br><img src="https://img-blog.csdnimg.cn/20200712131221612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70" alt="m0re" loading="lazy"><br>这是一张鲁班的图片，成功爬取下来了。</p>
<h2 id="参数解码和解码函数"><a href="#参数解码和解码函数" class="headerlink" title="参数解码和解码函数"></a>参数解码和解码函数</h2><p><code>urlencode</code>函数<br>用浏览器发送请求的时候，如果URL中包含了中文或者其他特殊字符，那么浏览器会自动的给它进行编码。而如果使用代码发送请求，那么就必须手动的进行编码，这个时候就应该使用<code>urlencode</code>函数来实现。<code>urlencode</code>可以把字典数据转换为URL编码的数据。示例代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line">data = &#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;m0re&#x27;</span>,<span class="string">&quot;age&quot;</span>:<span class="number">18</span>,<span class="string">&#x27;great&#x27;</span>:<span class="string">&#x27;hello,world&#x27;</span>&#125;</span><br><span class="line">j4y = parse.urlencode(data)</span><br><span class="line">print(j4y)</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">name=m0re&amp;age=<span class="number">18</span>&amp;great=hello%<span class="number">2</span>Cworld</span><br></pre></td></tr></table></figure>
<p>还可以这么做<br><img src="https://img-blog.csdnimg.cn/20200712134642641.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70" alt="m0re" loading="lazy"><br><code>parse_qs</code>函数可以将urlencode函数编码过的字符串进行解码<br>如下<br><img src="https://img-blog.csdnimg.cn/20200712135413295.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70" alt="m0re" loading="lazy"></p>
<h2 id="urlparse和urlsplit"><a href="#urlparse和urlsplit" class="headerlink" title="urlparse和urlsplit"></a><code>urlparse</code>和<code>urlsplit</code></h2><p>有时候拿到一个URL，想要对这个URL中的各个组成部分进行分割，那么这个时候就可以使用<code>urlparse</code>或者是<code>urlsplit</code>来进行分割。示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#encoding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com/s?username=zhiliao&#x27;</span></span><br><span class="line">result = parse.urlsplit(url)</span><br><span class="line">print(<span class="string">&#x27;scheme&#x27;</span>,result.scheme)</span><br><span class="line">print(<span class="string">&#x27;netloc&#x27;</span>,result.netloc)</span><br><span class="line">print(<span class="string">&#x27;path&#x27;</span>,result.path)</span><br><span class="line">print(<span class="string">&#x27;query&#x27;</span>,result.query)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200712140303705.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70" alt="m0re" loading="lazy"><br><code>urlparse</code>和<code>urlsplit</code>基本上是一模一样的，唯一不一样的地方是，<code>urlparse</code>里面多了一个<code>params</code>属性，而<code>urlsplit</code>没有这个属性。比如有一个<code>URL</code>为<code>http://www.baidu.com/s;hello?wd=python&amp;username=abc#1</code>，那么<code>urlparse</code>可以获取到<code>hello</code>，而<code>urlsplit</code>不可以获取到。但是<code>url</code>中的<code>params</code>也用的比较少。</p>
<h2 id="Request类"><a href="#Request类" class="headerlink" title="Request类"></a>Request类</h2><p>如果想要在请求的时候增加一些请求头，那么必须使用<code>request.Request</code>类来实现。比如要增加一个<code>User-Agent</code>，示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#encoding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.xxxxxx.com/zhaopin/Python/?labelWords=label&#x27;</span></span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&#x27;</span>&#125;</span><br><span class="line">req = request.Request(url,headers=headers)</span><br><span class="line">resp = request.urlopen(req)</span><br><span class="line">print(resp.readlines())</span><br></pre></td></tr></table></figure>
<p>可以看出也是可以爬取到的<br><img src="https://img-blog.csdnimg.cn/20200712143038154.png" alt="m0re" loading="lazy"><br>然后要记一点就是不加<code>User-Agent</code>的话，爬虫进行爬取的话，是不会得到有用的信息的，可以将上面的代码中的<code>user-Agent</code>去掉试试，如果不加<code>User-Agent</code>网站会轻易的识别出来爬虫，然后不给它爬取有用的信息。</p>
<p>到那时有些信息是需要爬取js代码才能得到的。比如爬取某网站的职位招聘信息。<br>先找到需要爬取的网站的以下信息：<br><img src="https://img-blog.csdnimg.cn/20200714172731396.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70" alt="m0re" loading="lazy"><br>首先找到信息所在的网页，查看请求的URL和请求方式</p>
<p>其次就是data数据<br><img src="https://img-blog.csdnimg.cn/20200714172919237.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70" alt="m0re" loading="lazy"><br>写入代码中去，模仿浏览器访问网页以达到爬取信息的目的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#encoding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,parse</span><br><span class="line"></span><br><span class="line"><span class="comment">#url = &#x27;https://www.xxxxx.com/jobs/list_python?labelWords=&amp;fromSearch=true&amp;suginput=&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#resp = request.urlopen(url)</span></span><br><span class="line"><span class="comment">#print(resp.read())</span></span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.xxxxxx.com/jobs/positionAjax.json?px=default&amp;city=%E5%8C%97%E4%BA%AC&amp;needAddtionalResult=false&#x27;</span></span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&#x27;</span> &#125;</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;first&#x27;</span>: <span class="string">&#x27;true&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;pn&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;kd&#x27;</span>: <span class="string">&#x27;python&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">req = request.Request(url, headers=headers, data=parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>), method=<span class="string">&#x27;POST&#x27;</span>)</span><br><span class="line">resp = request.urlopen(req)</span><br><span class="line">print(resp.read())</span><br></pre></td></tr></table></figure>
<p>爬取某招聘网站的职位信息，。这样已经是初步模型了，但是爬取不到信息，因为目前大多数网站都有反爬机制，所以报出了这样一个错误，其实也不是错误，就是浏览器返回的信息</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">b&#x27;&#123;&quot;status&quot;:false,&quot;msg&quot;:&quot;\xe6\x82\xa8\xe6\x93\x8d\xe4\xbd\x9c\xe5\xa4\xaa\xe9\xa2\x91\xe7\xb9\x81,\xe8\xaf\xb7\xe7\xa8\x8d\xe5\x90\x8e\xe5\x86\x8d\xe8\xae\xbf\xe9\x97\xae&quot;,&quot;clientIp&quot;:&quot;42.226.97.244&quot;,&quot;state&quot;:2402&#125;\n&#x27;</span></span><br></pre></td></tr></table></figure>
<p>这是经过URL编码的，解码一下就是访问的过于频繁，让我们稍后再访问。但是再回到浏览器中访问是完全没有问题的。<br>原因就是反爬虫机制的作用。还是能轻易检查出来我的爬虫，需要模仿的再像一点。加个Referer（同样是去浏览器的开发者界面寻找）<br>有些网站的反爬虫机制比较强，可能加了Referer还是爬取不到有用的信息。</p>
<p>需要再进行深层次的学习，添加其他验证身份的条件，比如下面将要学习的Cookie。</p>
<h2 id="ProxyHandler处理器（代理设置）"><a href="#ProxyHandler处理器（代理设置）" class="headerlink" title="ProxyHandler处理器（代理设置）"></a>ProxyHandler处理器（代理设置）</h2><p>很多网站会检测某一段时间某个IP的访问次数(通过流量统计，系统日志等)，如果访问次数多的不像正常人，它会禁止这个IP访问。所以应对方法是设置一些代理服务器，每隔一段时间换一个代理，就算IP地址被禁止，依然可以换个IP继续爬取。<br><code>urllib</code>中通过<code>ProxyHandler</code>来设置代理服务器。<br>前提：</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://httpbin.org/">http://httpbin.org/</a>这个都可以访问的。这个网站可以方便的查看http请求的一些参数</li>
<li>一个代理IP<br><img src="https://img-blog.csdnimg.cn/20200715184453382.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1ODM2NDc0,size_16,color_1FFFFF,t_70" alt="m0re" loading="lazy"><br>尽量选择一个最后验证时间比较近的。<br>代码示例：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#encoding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;http://httpbin.org/ip&quot;</span></span><br><span class="line"><span class="comment"># 传入代理</span></span><br><span class="line">handler = request.ProxyHandler(&#123;<span class="string">&quot;http&quot;</span>:<span class="string">&quot;49.70.89.14:9999&quot;</span>&#125;)</span><br><span class="line"><span class="comment"># 使用上面创建的handler构建一个opener</span></span><br><span class="line">opener = request.build_opener(handler)</span><br><span class="line"><span class="comment"># 使用opener去发送一个请求</span></span><br><span class="line">resp = opener.open(url)</span><br><span class="line">print(resp.read())</span><br></pre></td></tr></table></figure>
<ul>
<li><p>使用<code>urllib.request.ProxyHandler</code>传入一个代理，这个代理是一个字典，字典的key依赖于代理服务器能够接收的类型，一般是<code>http</code>或者<code>https</code></p>
</li>
<li><p>使用上一步创建的<code>headler</code>，以及<code>request.build_opener</code>创建一个<code>opener</code>对象</p>
</li>
<li><p>使用上一步创建的<code>opener</code>，调用<code>open</code>函数，发起请求</p>
</li>
</ul>
<p>==注意：==<br>代理需要自己设置有用的代理，我这个只是个免费的，现在可以，不知道明天还行不行，就这样。<br>然后各种报错，emmm，代码没问题，就是配置的问题了</p>
<p>报错一：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.error.URLError: &lt;urlopen error [Errno <span class="number">11001</span>] getaddrinfo failed&gt;</span><br></pre></td></tr></table></figure>
<p>这个与请求的URL有关，解决办法就是，把请求的URL原本是单引号包着的，改成双引号就OK了。(上面的代码已经修改)</p>
<p>报错二：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.error.URLError: &lt;urlopen error [WinError <span class="number">10061</span>] 由于目标计算机积极拒绝，无法连接。&gt;</span><br></pre></td></tr></table></figure>

<p>这个百度就有，但是有的解决不了，是电脑配置的原因。<br>可参考<br><a target="_blank" rel="noopener" href="https://bbs.csdn.net/topics/392282730?list=lz">https://bbs.csdn.net/topics/392282730?list=lz</a><br><a target="_blank" rel="noopener" href="http://www.manongjc.com/article/113971.html">http://www.manongjc.com/article/113971.html</a><br>配置问题，不细说了。<br><img src="https://img-blog.csdnimg.cn/20200715190503345.png" alt="m0re" loading="lazy"><br>下面挂几个常用的代理，如需更多请百度。<br>常用的代理有：</p>
<ul>
<li>快代理：<a target="_blank" rel="noopener" href="http://www.kuaidaili.com/">http://www.kuaidaili.com/</a></li>
<li><a target="_blank" rel="noopener" href="http://www.goubanjia.com/">http://www.goubanjia.com/</a></li>
<li><a target="_blank" rel="noopener" href="http://www.66ip.cn/1.html">http://www.66ip.cn/1.html</a></li>
<li><a target="_blank" rel="noopener" href="http://www.ip3366.net/free/">http://www.ip3366.net/free/</a></li>
<li><a target="_blank" rel="noopener" href="http://www.kxdaili.com/dailiip.html">http://www.kxdaili.com/dailiip.html</a></li>
<li><a target="_blank" rel="noopener" href="https://www.kuaidaili.com/free/">https://www.kuaidaili.com/free/</a></li>
<li><a target="_blank" rel="noopener" href="https://www.xicidaili.com/">https://www.xicidaili.com/</a></li>
<li><a target="_blank" rel="noopener" href="http://www.iphai.com/">http://www.iphai.com/</a></li>
<li><a target="_blank" rel="noopener" href="http://www.89ip.cn/">http://www.89ip.cn/</a></li>
<li><a target="_blank" rel="noopener" href="http://www.proxy360.cn/Region/China">http://www.proxy360.cn/Region/China</a></li>
<li><a target="_blank" rel="noopener" href="http://www.ip181.com/">http://www.ip181.com/</a></li>
<li><a target="_blank" rel="noopener" href="https://premproxy.com/">https://premproxy.com/</a></li>
<li><a target="_blank" rel="noopener" href="http://www.xroxy.com/">http://www.xroxy.com/</a></li>
<li><a target="_blank" rel="noopener" href="http://www.data5u.com/free/">http://www.data5u.com/free/</a><h2 id="Cookie模拟登陆"><a href="#Cookie模拟登陆" class="headerlink" title="Cookie模拟登陆"></a>Cookie模拟登陆</h2>在浏览器中寻找到cookie，加入到爬虫脚本中去。<br><img src="https://img-blog.csdnimg.cn/20200715180716730.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_bTByZQ==,size_16,color_1FFFFF,t_70" alt="m0re" loading="lazy"><br>添加到headers中，然后进行爬取。<br>以人人网为例。人人网中，要访问用户的主页进行浏览信息，需要登录才可以，也就是要有cookie信息，如果想用代码的方式访问，就需要有正确的cookie信息才能访问，解决方法有两种：第一种是使用浏览器访问，然后将cookie信息复制下来，放到headers中。（没有账号，懒得注册了）缺点是每次访问都需要从浏览器复制cookie，麻烦。在Python处理Cookie，一般是通过<code>http.cookiejar</code>模块和<code>urllib</code>模块的<code>HTTPCookieProcessor</code>处理器类一起使用。<code>http.cookiejar</code>模块的作用是提供用于存储cookie的对象。而<code>HTTPCookieProcessor</code>处理器主要作用是处理这些cookie对象，并构建handler对象。<h2 id="爬虫自动登录访问授权页面"><a href="#爬虫自动登录访问授权页面" class="headerlink" title="爬虫自动登录访问授权页面"></a>爬虫自动登录访问授权页面</h2><code>http.cookiejar</code>模块<br>该模块主要的类有：CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。这四个类的作用分别如下</li>
</ul>
<ol>
<li><code>CookieJar</code>：管理HTTP cookie值、存储HTTP请求生成的cookie、向传出的HTTP请求添加cookie的对象。整个cookie都存储在内存中，对CookieJar实例进行垃圾回收后cookie也将丢失。</li>
<li><code>FileCookieJar</code>：从<code>CookieJar</code>派生而来，用来创建<code>FileCookieJar</code>实例，检索cookie信息并将cookie存储到文件中。<code>filename</code>是存储cookie的文件名。<code>delayload</code>为True时支持延迟访问文件，即只有在需要时才读取文件或在文件中存储数据。</li>
<li><code>MozillaCookieJar</code>：从<code>FileCookieJar</code>派生而来，创建与Mozilla浏览器cookie.txt兼容的<code>FileCookieJar</code>实例。</li>
<li><code>LWPCookieJar</code>：从<code>FileCookieJar</code>派生而来，创建与libwww-per标准的Set-Cookie3文件格式兼容的<code>FileCookieJar</code>实例。</li>
</ol>
<p>先学习到这里，需要消化一下，下次继续学习这个部分——利用爬虫登录登陆访问。</p>
</div><div id="reward-container"><span class="hty-icon-button button-glow" id="reward-button" title="打赏" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === &quot;none&quot;) ? &quot;block&quot; : &quot;none&quot;;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span><div id="reward-comment">I'm so cute. Please give me a candy.</div><div id="qr" style="display:none;"><div style="display:inline-block"><a href="/img/alipay.jpg"><img loading="lazy" src="/img/alipay.jpg" alt="支付宝" title="支付宝"></a><div><span style="color:#00A3EE"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-alipay-line"></use></svg></span></div></div><div style="display:inline-block"><a href="/img/wechatpay.jpg"><img loading="lazy" src="/img/wechatpay.jpg" alt="微信支付" title="微信支付"></a><div><span style="color:#2DC100"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wechat-pay-line"></use></svg></span></div></div></div></div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>m0re</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://m0re.top/posts/7bd464e8/" title="python分布式爬虫————爬虫前奏和网络请求">https://m0re.top/posts/7bd464e8/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/posts/94be70cd/" rel="prev" title="python脚本编程的知识点"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">python脚本编程的知识点</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/posts/1c4a3d56/" rel="next" title="PHP序列化与反序列化、PHP伪协议"><span class="post-nav-text">PHP序列化与反序列化、PHP伪协议</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div id="comment"><div class="comment-tooltip text-center"><span>尝试使用 utterances （基于 GitHub Issues）评论系统。</span><br><span>您可以点击下方按钮切换对应评论系统，</span><br><span><script>function showUtterances(){ document.querySelector(".utterances").style.display='block'; document.querySelector('#valine-container').style.display='none'; }function showValine(){ document.querySelector('#valine-container').style.display='block'; document.querySelector(".utterances").style.display='none'; }</script></span><br><span><a class="hty-button hty-button--raised" onclick="showValine()">Valine</a><a class="hty-button hty-button--raised" onclick="showUtterances()">utterances</a></span><br><span><style>.utterances{display:none;}</style></span><br></div><div id="valine-container"></div><script src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script>function initValine() {
  const valineConfig = {"enable":true,"appId":"gUEA9zKLhCnMFxQI5kI20WiD-gzGzoHsz","appKey":"GQKAp04LNloy3MrDMv86MI3i","placeholder":"评论一下~&nbsp_(:з」∠)_ （填写邮箱可以收到回复通知～）","avatar":null,"pageSize":10,"visitor":true,"highlight":true,"recordIP":true,"enableQQ":true,"requiredFields":["nick","mail"],"el":"#valine-container","lang":"zh-cn"}
  valineConfig.path = window.location.pathname
  new Valine(valineConfig)
}
setTimeout(initValine, 1000)</script></div></main><footer class="sidebar-translate" id="footer"><div class="beian"><a rel="noopener" href="http://www.beian.miit.gov.cn" target="_blank">豫ICP备2020025591号-1</a></div><div class="copyright"><span>&copy; 2019 – 2020 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> m0re</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v5.1.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v0.9.7</span></div><div class="live_time"><span>本博客已苟延残喘了</span><span id="display_live_time"></span><span class="moe-text">(●'◡'●)</span><script>function blog_live_time() {
  window.setTimeout(blog_live_time, 1000);
  const start = new Date('2020-02-21T00:00:00');
  const now = new Date();
  const timeDiff = (now.getTime() - start.getTime());
  const msPerMinute = 60 * 1000;
  const msPerHour = 60 * msPerMinute;
  const msPerDay = 24 * msPerHour;
  const passDay = Math.floor(timeDiff / msPerDay);
  const passHour = Math.floor((timeDiff % msPerDay) / 60 / 60 / 1000);
  const passMinute = Math.floor((timeDiff % msPerHour) / 60 / 1000);
  const passSecond = Math.floor((timeDiff % msPerMinute) / 1000);
  display_live_time.innerHTML = " " + passDay + " 天 " + passHour + " 小时 " + passMinute + " 分 " + passSecond + " 秒";
}
blog_live_time();
</script></div></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" href="javascript:;" title="搜索"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-search-line"></use></svg></span></a><script>window.addEventListener("DOMContentLoaded", () => {
  // Handle and trigger popup window
  document.querySelector(".popup-trigger").addEventListener("click", () => {
    document.querySelector(".popup").classList.add("show");
    setTimeout(() => {
      document.querySelector(".search-input").focus();
    }, 100);
  });

  // Monitor main search box
  const onPopupClose = () => {
    document.querySelector(".popup").classList.remove("show");
  };

  document.querySelector(".popup-btn-close").addEventListener("click", () => {
    onPopupClose();
  });

  window.addEventListener("keyup", event => {
    if (event.key === "Escape") {
      onPopupClose();
    }
  });
});
</script><script src="/js/search/local-search.js" defer></script><div class="popup search-popup"><div class="search-header"><span class="popup-btn-close close-icon hty-icon-button"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-close-line"></use></svg></span></div><div class="search-input-container"><input class="search-input" id="local-search-input" type="text" placeholder="这里可以搜索本站内容~" value=""></div><div id="local-search-result"></div></div></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script><script src="https://cdn.jsdelivr.net/npm/live2d-widget@^3.1.3/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"https://cdn.jsdelivr.net/npm/live2d-widget-model-wanko@1.0.5/assets/wanko.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body></html>